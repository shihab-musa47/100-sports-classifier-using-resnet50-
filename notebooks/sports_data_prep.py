# -*- coding: utf-8 -*-
"""sports_data_prep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sL-SuPlItOu_wyeA-kRRbnYplaLpK8Jk
"""

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline
bs = 8 # batch size

!pip install -Uqq fastai fastbook nbdev

from fastai import *
from fastbook import *
from fastai.vision.all import *

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/capstone2/sports image classifier

!zip -r '/content/drive/My Drive/capstone2/Automated Diabetic Retinopathy & Macular Edema Triage/data'

doc(search_images_ddg)

sports_labels = [
    "Air Hockey",
    "Amputee Football",
    "Archery",
    "Arm Wrestling",
    "Axe Throwing",
    "Balance Beam",
    "Barrel Racing",
    "Baseball",
    "Basketball",
    "Baton Twirling",
    "Bike Polo",
    "Billiards",
    "BMX",
    "Bobsled",
    "Bowling",
    "Boxing",
    "Bull Riding",
    "Bungee Jumping",
    "Canoe Slalom",
    "Cheerleading",
    "Chuckwagon Racing",
    "Cricket",
    "Croquet",
    "Curling",
    "Disc Golf",
    "Fencing",
    "Field Hockey",
    "Figure Skating (Men)",
    "Figure Skating (Women)",
    "Figure Skating (Pairs)",
    "Fly Fishing",
    "Football (Soccer)",
    "Formula 1 Racing",
    "Frisbee",
    "Gaga",
    "Giant Slalom",
    "Golf",
    "Hammer Throw",
    "Hang Gliding",
    "Harness Racing",
    "High Jump",
    "Ice Hockey",
    "Horse Jumping",
    "Horse Racing",
    "Horseshoe Pitching",
    "Hurdles",
    "Hydroplane Racing",
    "Ice Climbing",
    "Ice Yachting",
    "Jai Alai",
    "Javelin",
    "Jousting",
    "Judo",
    "Lacrosse",
    "Log Rolling",
    "Luge",
    "Motorcycle Racing",
    "Mushing",
    "NASCAR Racing",
    "Olympic Wrestling",
    "Parallel Bars",
    "Pole Climbing",
    "Pole Dancing",
    "Pole Vault",
    "Polo",
    "Pommel Horse",
    "Rings (Gymnastics)",
    "Rock Climbing",
    "Roller Derby",
    "Rollerblade Racing",
    "Rowing",
    "Rugby",
    "Sailboat Racing",
    "Shot Put",
    "Shuffleboard",
    "Sidecar Racing",
    "Ski Jumping",
    "Sky Surfing",
    "Skydiving",
    "Snowboarding",
    "Snowmobile Racing",
    "Speed Skating",
    "Steer Wrestling",
    "Sumo Wrestling",
    "Surfing",
    "Swimming",
    "Table Tennis",
    "Tennis",
    "Track Cycling",
    "Trapeze",
    "Tug of War",
    "Ultimate Frisbee",
    "Uneven Bars",
    "Volleyball",
    "Water Cycling",
    "Water Polo",
    "Weightlifting",
    "Wheelchair Basketball",
    "Wheelchair Racing",
    "Wingsuit Flying"
]

len(sports_labels)

images = search_images_ddg(sports_labels[0])
f"No of Images => {len(images)} -- One Image URL => {images[0]}"

# # Setup
# data_path = "data"
# if not os.path.exists(data_path):
#     os.mkdir(data_path)

# # Limit image downloads to 50 per label
# max_images_per_label = 50

# # Loop through sports labels and download images
# for sport in sports_labels:
#     dest = f"{data_path}/{sport}"
#     if not os.path.exists(dest):
#         os.mkdir(dest)

#     try:
#         sport_image_urls = search_images_ddg(sport, max_images=max_images_per_label)
#         download_images(dest, urls=sport_image_urls)
#     except Exception as e:
#         print(f"Failed for {sport}: {e}")
#         continue

# Run this to diagnose the issue
import urllib.request

print("Testing connection...")
try:
    response = urllib.request.urlopen('https://duckduckgo.com', timeout=5)
    print(f"‚úÖ DuckDuckGo reachable: {response.status}")
except Exception as e:
    print(f"‚ùå Cannot reach DuckDuckGo: {e}")

# Test the actual search function
try:
    print("Testing image search...")
    urls = search_images_ddg("tennis", max_images=5)
    print(f"‚úÖ Found {len(urls)} URLs")
except Exception as e:
    print(f"‚ùå Search failed: {e}")

pip install bing-image-downloader

from bing_image_downloader import downloader
import os
from datetime import datetime

data_path = "data"
if not os.path.exists(data_path):
    os.mkdir(data_path)

start_time = datetime.now()
successful = 0
failed = 0

print("=" * 60)
print(f"üèÖ DOWNLOADING WITH BING IMAGE SEARCH")
print("=" * 60)

for idx, sport in enumerate(sports_labels, 1):
    print(f"[{idx}/{len(sports_labels)}] {sport[:30]:<30}", end=" ", flush=True)
    try:
        downloader.download(
            sport,
            limit=50,
            output_dir=data_path,
            adult_filter_off=True,
            force_replace=False,
            timeout=15,
            verbose=False
        )
        successful += 1
        print("‚úÖ")
    except Exception as e:
        failed += 1
        print(f"‚ùå")

print("=" * 60)
print(f"‚úÖ {successful} | ‚ùå {failed} | ‚è±Ô∏è {(datetime.now()-start_time).total_seconds()/60:.1f} min")

data_path = "data"

image_counts = get_image_files(data_path)
image_counts

failed = verify_images(image_counts)
failed

failed.map(Path.unlink)

doc(get_image_files)

dblock = DataBlock(
    blocks=(ImageBlock, CategoryBlock),
    get_items=get_image_files,                        # get_image_files returns a list of all images in that path recursively by default
    splitter=RandomSplitter(valid_pct=0.1, seed=42),  # getting 90-10 train-validation split
    get_y=parent_label,                               # taking the folder name as labels
    item_tfms=Resize(128))                            # resizing to get the image of same shape

dls = dblock.dataloaders(data_path,bs=bs)             # setting up batch size for efficient GPU use

dls.train.show_batch(max_n=8, nrows=2)

dls.valid.show_batch(max_n=8, nrows=2)

dblock = dblock.new(item_tfms=Resize(128, ResizeMethod.Squish)) # Squishing the whole image into (128 x 128) size
dls = dblock.dataloaders(data_path)
dls.valid.show_batch(max_n=4, nrows=1)

dblock = dblock.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) # Randomly cropping, cutting, resizing, coloring parts of image into (128 x 128) size
dls = dblock.dataloaders(data_path)
dls.valid.show_batch(max_n=4, nrows=1)

doc(aug_transforms)

# RandomResizedCrop crops images randomly and create copies so that we don't miss out anything
# aug_transforms is used for image data augmentation
dblock = dblock.new(item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms())
dls = dblock.dataloaders(data_path)
dls.train.show_batch(max_n=8, nrows=2)

torch.save(dls, "sports_dataloader_v0.pkl")

